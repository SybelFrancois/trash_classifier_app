{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-15T16:49:55.504102Z",
     "end_time": "2023-04-15T16:50:00.252902Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://github.com/garythung/trashnet/raw/master/data/dataset-resized.zip\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"dataset-resized.zip\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the dataset\n",
    "with zipfile.ZipFile(\"dataset-resized.zip\", \"r\") as z:\n",
    "    z.extractall(\"trashnet_dataset\")\n",
    "\n",
    "# Define the categories and directory structure\n",
    "categories = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "base_dir = \"trashnet_dataset/dataset-resized\"\n",
    "train_dir = \"trashnet_dataset/train\"\n",
    "val_dir = \"trashnet_dataset/val\"\n",
    "test_dir = \"trashnet_dataset/test\"\n",
    "\n",
    "# Create the directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Split ratio\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Split the dataset\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
    "\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    images = os.listdir(category_dir)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    train_size = int(len(images) * train_ratio)\n",
    "    val_size = int(len(images) * val_ratio)\n",
    "\n",
    "    train_images = images[:train_size]\n",
    "    val_images = images[train_size:train_size + val_size]\n",
    "    test_images = images[train_size + val_size:]\n",
    "\n",
    "    for image in train_images:\n",
    "        shutil.copy(os.path.join(category_dir, image), os.path.join(train_dir, category, image))\n",
    "    for image in val_images:\n",
    "        shutil.copy(os.path.join(category_dir, image), os.path.join(val_dir, category, image))\n",
    "    for image in test_images:\n",
    "        shutil.copy(os.path.join(category_dir, image), os.path.join(test_dir, category, image))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 16:58:40.866327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2019 images belonging to 6 classes.\n",
      "Found 251 images belonging to 6 classes.\n",
      "Found 257 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the image size and batch size\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Create an ImageDataGenerator for the training set with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Create an ImageDataGenerator for the validation and testing sets without data augmentation\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load and preprocess the training set\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"trashnet_dataset/train\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load and preprocess the validation set\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    \"trashnet_dataset/val\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load and preprocess the testing set\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    \"trashnet_dataset/test\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T16:58:25.534364Z",
     "end_time": "2023-04-15T16:58:59.167298Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 17:03:26.244538: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 2.0036 - accuracy: 0.3051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 17:04:42.296294: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 78s 1s/step - loss: 2.0036 - accuracy: 0.3051 - val_loss: 1.4243 - val_accuracy: 0.4183\n",
      "Epoch 2/25\n",
      "64/64 [==============================] - 76s 1s/step - loss: 1.3859 - accuracy: 0.4364 - val_loss: 1.1985 - val_accuracy: 0.5618\n",
      "Epoch 3/25\n",
      "64/64 [==============================] - 77s 1s/step - loss: 1.2538 - accuracy: 0.5027 - val_loss: 1.0955 - val_accuracy: 0.5817\n",
      "Epoch 4/25\n",
      "64/64 [==============================] - 76s 1s/step - loss: 1.2072 - accuracy: 0.5290 - val_loss: 1.1871 - val_accuracy: 0.5777\n",
      "Epoch 5/25\n",
      "64/64 [==============================] - 74s 1s/step - loss: 1.1486 - accuracy: 0.5617 - val_loss: 1.2838 - val_accuracy: 0.5777\n",
      "Epoch 6/25\n",
      "64/64 [==============================] - 75s 1s/step - loss: 1.1834 - accuracy: 0.5453 - val_loss: 1.1033 - val_accuracy: 0.5936\n",
      "Epoch 7/25\n",
      "64/64 [==============================] - 75s 1s/step - loss: 1.0823 - accuracy: 0.5864 - val_loss: 1.0006 - val_accuracy: 0.6295\n",
      "Epoch 8/25\n",
      "64/64 [==============================] - 76s 1s/step - loss: 1.0307 - accuracy: 0.6132 - val_loss: 1.0771 - val_accuracy: 0.6175\n",
      "Epoch 9/25\n",
      "64/64 [==============================] - 75s 1s/step - loss: 1.0300 - accuracy: 0.6062 - val_loss: 0.8789 - val_accuracy: 0.6693\n",
      "Epoch 10/25\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.9753 - accuracy: 0.6315 - val_loss: 0.9716 - val_accuracy: 0.6056\n",
      "Epoch 11/25\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.9576 - accuracy: 0.6379 - val_loss: 0.9387 - val_accuracy: 0.6375\n",
      "Epoch 12/25\n",
      "64/64 [==============================] - 73s 1s/step - loss: 0.9069 - accuracy: 0.6587 - val_loss: 0.9015 - val_accuracy: 0.6135\n",
      "Epoch 13/25\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.9258 - accuracy: 0.6677 - val_loss: 0.8141 - val_accuracy: 0.7092\n",
      "Epoch 14/25\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.9031 - accuracy: 0.6677 - val_loss: 0.8955 - val_accuracy: 0.6813\n",
      "Epoch 15/25\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.8530 - accuracy: 0.6914 - val_loss: 0.9017 - val_accuracy: 0.6653\n",
      "Epoch 16/25\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.8342 - accuracy: 0.7053 - val_loss: 0.8464 - val_accuracy: 0.6932\n",
      "Epoch 17/25\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.8013 - accuracy: 0.6989 - val_loss: 0.8778 - val_accuracy: 0.7131\n",
      "Epoch 18/25\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.8052 - accuracy: 0.7043 - val_loss: 0.8417 - val_accuracy: 0.7211\n",
      "Epoch 19/25\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7942 - accuracy: 0.7093 - val_loss: 0.8431 - val_accuracy: 0.7012\n",
      "Epoch 20/25\n",
      "64/64 [==============================] - 73s 1s/step - loss: 0.7572 - accuracy: 0.7320 - val_loss: 0.8167 - val_accuracy: 0.7012\n",
      "Epoch 21/25\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7044 - accuracy: 0.7449 - val_loss: 0.8250 - val_accuracy: 0.7171\n",
      "Epoch 22/25\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.7789 - accuracy: 0.7221 - val_loss: 0.8071 - val_accuracy: 0.7211\n",
      "Epoch 23/25\n",
      "64/64 [==============================] - 70s 1s/step - loss: 0.7066 - accuracy: 0.7444 - val_loss: 0.8912 - val_accuracy: 0.7171\n",
      "Epoch 24/25\n",
      "64/64 [==============================] - 69s 1s/step - loss: 0.7199 - accuracy: 0.7335 - val_loss: 0.7917 - val_accuracy: 0.7331\n",
      "Epoch 25/25\n",
      "64/64 [==============================] - 70s 1s/step - loss: 0.6640 - accuracy: 0.7543 - val_loss: 0.8139 - val_accuracy: 0.7371\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(val_generator)\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('trash_classifier.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:03:25.045355Z",
     "end_time": "2023-04-15T17:34:05.650209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 17:36:36.613724: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 3s 248ms/step - loss: 0.8220 - accuracy: 0.7354\n",
      "Test loss: 0.8220\n",
      "Test accuracy: 0.7354\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator))\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:36:36.464472Z",
     "end_time": "2023-04-15T17:36:39.227359Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Collecting Trash Can Images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[H\u001B[2J--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        Automatic download ... 24 out of 24 (100%)                             \n",
      "        Automatic download ... 4411031 out of 4411031 (100%)                   \n",
      "        Automatic download ... 1000 out of 1000 (100%)                         \n",
      "    Formation of labels ... 1000 out of 1000 (100%)                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\u001B[1m\u001B[94mOIDv6 - Downloading single or multiple classes from the Open Images V6 Dataset ...\u001B[0m\n",
      "[2023-04-15 18:50:30] Start: \n",
      "\tAuthor: Dmitry Ryumin\n",
      "\tEmail: dl_03.03.1991@mail.ru\n",
      "\tMaintainer: Dmitry Ryumin\n",
      "\tVersion: 1.0.5\n",
      "[2023-04-15 18:50:30] Checking command line arguments for validity ...\n",
      "[2023-04-15 18:50:30] Creating Directories for Metadata ...\n",
      "[2023-04-15 18:50:30] Downloading \"waste container\" ...\n",
      "    File \"class-descriptions-boxable.csv\" not found ...\n",
      "    File \"oidv6-train-annotations-bbox.csv\" not found ...\n",
      "    Extracting data from \"oidv6-train-annotations-bbox.csv\" ...\n",
      "    Total \"train\" images 1020 ... of which will be loaded 1000 ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def download_open_images_dataset(class_name, limit, dataset_dir):\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"oidv6\",\n",
    "            \"downloader\",\n",
    "            \"--dataset\",\n",
    "            \"train\",\n",
    "            \"--classes\",\n",
    "            class_name,\n",
    "            \"--type_csv\",\n",
    "            \"train\",\n",
    "            \"--limit\",\n",
    "            str(limit),\n",
    "            \"--multiclasses\",\n",
    "            \"1\",\n",
    "            \"--download_folder\",\n",
    "            dataset_dir,\n",
    "            \"--yes\",  # Add the --yes flag to automatically confirm prompts\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the class name, number of images to download, and the dataset directory\n",
    "    class_name = \"Waste container\"\n",
    "    limit = 1000\n",
    "    dataset_dir = \"trash_cans_dataset\"\n",
    "\n",
    "    # Create the dataset directory if it doesn't exist\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download the images from the Open Images dataset\n",
    "    download_open_images_dataset(class_name, limit, dataset_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T18:50:28.917509Z",
     "end_time": "2023-04-15T19:27:02.356333Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
